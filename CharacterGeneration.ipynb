{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'alice_adventures_book.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fileName) as fil:\n",
    "    text = fil.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars 163817 Unique chars 61\n"
     ]
    }
   ],
   "source": [
    "print('Total chars', len(text),  'Unique chars', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_int = dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '#': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " '*': 8,\n",
       " ',': 9,\n",
       " '-': 10,\n",
       " '.': 11,\n",
       " '/': 12,\n",
       " '0': 13,\n",
       " '1': 14,\n",
       " '2': 15,\n",
       " '3': 16,\n",
       " '4': 17,\n",
       " '5': 18,\n",
       " '6': 19,\n",
       " '7': 20,\n",
       " '8': 21,\n",
       " '9': 22,\n",
       " ':': 23,\n",
       " ';': 24,\n",
       " '?': 25,\n",
       " '@': 26,\n",
       " '[': 27,\n",
       " ']': 28,\n",
       " '_': 29,\n",
       " 'a': 30,\n",
       " 'b': 31,\n",
       " 'c': 32,\n",
       " 'd': 33,\n",
       " 'e': 34,\n",
       " 'f': 35,\n",
       " 'g': 36,\n",
       " 'h': 37,\n",
       " 'i': 38,\n",
       " 'j': 39,\n",
       " 'k': 40,\n",
       " 'l': 41,\n",
       " 'm': 42,\n",
       " 'n': 43,\n",
       " 'o': 44,\n",
       " 'p': 45,\n",
       " 'q': 46,\n",
       " 'r': 47,\n",
       " 's': 48,\n",
       " 't': 49,\n",
       " 'u': 50,\n",
       " 'v': 51,\n",
       " 'w': 52,\n",
       " 'x': 53,\n",
       " 'y': 54,\n",
       " 'z': 55,\n",
       " 'â€˜': 56,\n",
       " 'â€™': 57,\n",
       " 'â€œ': 58,\n",
       " 'â€': 59,\n",
       " '\\ufeff': 60}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_int = [chars_to_int[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 45, 47, 44, 39, 34, 32, 49, 1, 36]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will consider previous 100 chars to predict next char\n",
    "sequence_size = 100\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(text_to_int)-sequence_size):\n",
    "    x.append(text_to_int[i:i+sequence_size])\n",
    "    y.append(text_to_int[i+sequence_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training we would like to have y as a one-hot vector\n",
    "Y = np_utils.to_categorical(y)\n",
    "X = np.reshape(x, (len(x), sequence_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM layer input_shape = (#samples, #timeSteps, #features)   \n",
    "\n",
    "\n",
    "\n",
    "In Keras LSTM(n) means \"create an LSTM layer consisting of LSTM units. The following picture demonstrates what layer and unit (or neuron) are, and the rightmost image shows the internal structure of a single LSTM unit.\n",
    "\n",
    "![image1.png](./image1.png)\n",
    "\n",
    "The following picture shows how the whole LSTM layer operates.\n",
    "\n",
    "\n",
    "\n",
    "As we know an LSTM layer processes a sequence, i.e, ğ•©1,â€¦,ğ•©ğ‘\n",
    ". At each step ğ‘¡ the layer (each neuron) takes the input ğ•©ğ•¥, output from previous step ğ•™ğ•¥âˆ’ğŸ™, and bias ğ‘, and outputs a vector ğ•™ğ•¥. Coordinates of ğ•™ğ•¥ are outputs of the neurons/units, and hence the size of the vector ğ•™ğ•¥ is equal to the number of units/neurons. This process continues until ğ•©ğ‘\n",
    "\n",
    "![image2.png](./image2.png)\n",
    "\n",
    "Now let's compute the number of parameters for LSTM(1) and LSTM(3) and compare it with what Keras shows when we call model.summary().\n",
    "\n",
    "Let ğ‘–ğ‘›ğ‘\n",
    "be the size of the vector ğ•©ğ•¥ and ğ‘œğ‘¢ğ‘¡ be the size of the vector ğ•™ğ•¥ (this is also the number of neurons/units). Each neuron/unit takes input vector, output from the previous step, and a bias which makes ğ‘–ğ‘›ğ‘+ğ‘œğ‘¢ğ‘¡+1 parameters (weights). But we have ğ‘œğ‘¢ğ‘¡ number of neurons and so we have ğ‘œğ‘¢ğ‘¡Ã—(ğ‘–ğ‘›ğ‘+ğ‘œğ‘¢ğ‘¡+1) parameters. Finally each unit has 4 weights (see the rightmost image, yellow boxes) and we have the following formula for the number of parameters:\n",
    "4ğ‘œğ‘¢ğ‘¡(ğ‘–ğ‘›ğ‘+ğ‘œğ‘¢ğ‘¡+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(350, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(350))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 350)          492800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 350)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 350)               981400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                21060     \n",
      "=================================================================\n",
      "Total params: 1,495,260\n",
      "Trainable params: 1,495,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=\"weights-{epoch:02d}-{loss:.4f}.hdf5\", monitor=\"loss\", save_best_only=True, mode=\"min\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y, epochs=20, batch_size=128, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reference* : https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
