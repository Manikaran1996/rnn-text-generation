{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'alice_adventures_book.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fileName) as fil:\n",
    "    text = fil.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars 163817 Unique chars 61\n"
     ]
    }
   ],
   "source": [
    "print('Total chars', len(text),  'Unique chars', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_int = dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '#': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " '*': 8,\n",
       " ',': 9,\n",
       " '-': 10,\n",
       " '.': 11,\n",
       " '/': 12,\n",
       " '0': 13,\n",
       " '1': 14,\n",
       " '2': 15,\n",
       " '3': 16,\n",
       " '4': 17,\n",
       " '5': 18,\n",
       " '6': 19,\n",
       " '7': 20,\n",
       " '8': 21,\n",
       " '9': 22,\n",
       " ':': 23,\n",
       " ';': 24,\n",
       " '?': 25,\n",
       " '@': 26,\n",
       " '[': 27,\n",
       " ']': 28,\n",
       " '_': 29,\n",
       " 'a': 30,\n",
       " 'b': 31,\n",
       " 'c': 32,\n",
       " 'd': 33,\n",
       " 'e': 34,\n",
       " 'f': 35,\n",
       " 'g': 36,\n",
       " 'h': 37,\n",
       " 'i': 38,\n",
       " 'j': 39,\n",
       " 'k': 40,\n",
       " 'l': 41,\n",
       " 'm': 42,\n",
       " 'n': 43,\n",
       " 'o': 44,\n",
       " 'p': 45,\n",
       " 'q': 46,\n",
       " 'r': 47,\n",
       " 's': 48,\n",
       " 't': 49,\n",
       " 'u': 50,\n",
       " 'v': 51,\n",
       " 'w': 52,\n",
       " 'x': 53,\n",
       " 'y': 54,\n",
       " 'z': 55,\n",
       " '‘': 56,\n",
       " '’': 57,\n",
       " '“': 58,\n",
       " '”': 59,\n",
       " '\\ufeff': 60}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_int = [chars_to_int[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 45, 47, 44, 39, 34, 32, 49, 1, 36]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will consider previous 100 chars to predict next char\n",
    "sequence_size = 100\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(text_to_int)-sequence_size):\n",
    "    x.append(text_to_int[i:i+sequence_size])\n",
    "    y.append(text_to_int[i+sequence_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training we would like to have y as a one-hot vector\n",
    "Y = np_utils.to_categorical(y)\n",
    "X = np.reshape(x, (len(x), sequence_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM layer input_shape = (#samples, #timeSteps, #features)   \n",
    "\n",
    "\n",
    "\n",
    "In Keras LSTM(n) means \"create an LSTM layer consisting of LSTM units. The following picture demonstrates what layer and unit (or neuron) are, and the rightmost image shows the internal structure of a single LSTM unit.\n",
    "\n",
    "![image1.png](./image1.png)\n",
    "\n",
    "The following picture shows how the whole LSTM layer operates.\n",
    "\n",
    "\n",
    "\n",
    "As we know an LSTM layer processes a sequence, i.e, 𝕩1,…,𝕩𝑁\n",
    ". At each step 𝑡 the layer (each neuron) takes the input 𝕩𝕥, output from previous step 𝕙𝕥−𝟙, and bias 𝑏, and outputs a vector 𝕙𝕥. Coordinates of 𝕙𝕥 are outputs of the neurons/units, and hence the size of the vector 𝕙𝕥 is equal to the number of units/neurons. This process continues until 𝕩𝑁\n",
    "\n",
    "![image2.png](./image2.png)\n",
    "\n",
    "Now let's compute the number of parameters for LSTM(1) and LSTM(3) and compare it with what Keras shows when we call model.summary().\n",
    "\n",
    "Let 𝑖𝑛𝑝\n",
    "be the size of the vector 𝕩𝕥 and 𝑜𝑢𝑡 be the size of the vector 𝕙𝕥 (this is also the number of neurons/units). Each neuron/unit takes input vector, output from the previous step, and a bias which makes 𝑖𝑛𝑝+𝑜𝑢𝑡+1 parameters (weights). But we have 𝑜𝑢𝑡 number of neurons and so we have 𝑜𝑢𝑡×(𝑖𝑛𝑝+𝑜𝑢𝑡+1) parameters. Finally each unit has 4 weights (see the rightmost image, yellow boxes) and we have the following formula for the number of parameters:\n",
    "4𝑜𝑢𝑡(𝑖𝑛𝑝+𝑜𝑢𝑡+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(350, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(350))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 350)          492800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 350)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 350)               981400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                21060     \n",
      "=================================================================\n",
      "Total params: 1,495,260\n",
      "Trainable params: 1,495,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=\"weights-{epoch:02d}-{loss:.4f}.hdf5\", monitor=\"loss\", save_best_only=True, mode=\"min\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y, epochs=20, batch_size=128, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reference* : https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
